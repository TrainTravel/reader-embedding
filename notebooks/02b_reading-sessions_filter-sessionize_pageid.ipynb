{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "filter sessions and write to txt-file on disc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f501eb79748>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_config = {}\n",
    "spark_config = {}\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.5'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark\n",
    "\n",
    "## regular\n",
    "\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"2g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 256\n",
    "# }\n",
    "# ## big\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"4g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 128,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 512\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def blacklist_qids_to_pageids(blacklist_qids,wiki):\n",
    "    '''\n",
    "    transform a list of qids into pageids for a given wiki.\n",
    "    <30!\n",
    "    '''\n",
    "    ## get the page-titles\n",
    "    api_url_base = 'https://wikidata.org/w/api.php'\n",
    "    params = {\n",
    "                'action':'wbgetentities',\n",
    "                'props':'sitelinks/urls',\n",
    "                'languages':'en',\n",
    "                'format' : 'json',\n",
    "                'sitefilter':wiki,\n",
    "                'ids':'|'.join(blacklist_qids),\n",
    "            }\n",
    "    blacklist_titles = []\n",
    "    try:\n",
    "        response = requests.get( api_url_base,params=params).json()\n",
    "        if 'entities' in response:\n",
    "            for qid, qid_dict in response['entities'].items():\n",
    "                title = qid_dict.get('sitelinks',{}).get(wiki,{}).get('title','').replace(' ','_')\n",
    "                if len(title)>0:\n",
    "                    blacklist_titles += [title]\n",
    "    except:\n",
    "        pass\n",
    "    blacklist_pageids = []\n",
    "    if len(blacklist_titles)>0:\n",
    "\n",
    "        ## get the page-ids\n",
    "        api_url_base = 'https://%s.wikipedia.org/w/api.php'%( wiki.replace('wiki','') )\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"titles\": \"|\".join([str(h) for h in blacklist_titles]),\n",
    "            \"prop\": \"pageprops\",\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "    try:\n",
    "        response = requests.get( api_url_base,params=params).json()\n",
    "        if 'query' in response:\n",
    "            if 'pages' in response['query']:\n",
    "                for page, page_dict in response['query']['pages'].items():\n",
    "                    blacklist_pageids += [int(page)]\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return blacklist_pageids\n",
    "\n",
    "\n",
    "## defining filter and maps\n",
    "def parse_requests(requests):\n",
    "    \"\"\"\n",
    "    do some initial parsing:\n",
    "    - drop pages without timestamp (we dont know which order)\n",
    "    \"\"\"\n",
    "    requests_clean = []\n",
    "    for r in requests:\n",
    "        if r['ts'] == None:\n",
    "            pass\n",
    "        else:\n",
    "            requests_clean += [r]\n",
    "    return requests_clean\n",
    "\n",
    "def filter_consecutive_articles(requests):\n",
    "    \"\"\"\n",
    "    Looking at the data, there are a lot of\n",
    "    sessions with the same article\n",
    "    requested 2 times in a row. This\n",
    "    does not make sense for training, so\n",
    "    lets collapse them into 1 request\n",
    "    \"\"\"\n",
    "    r = requests[0]\n",
    "    t = r['page_title']\n",
    "    clean_rs = [r,]\n",
    "    prev_t = t\n",
    "    for r in requests[1:]:\n",
    "        t = r['page_title']\n",
    "        if t == prev_t:\n",
    "            continue\n",
    "        else:\n",
    "            clean_rs.append(r)\n",
    "            prev_t = t\n",
    "    return clean_rs\n",
    "\n",
    "def filter_blacklist_pageid(requests):\n",
    "    \"\"\"\n",
    "    If the session contains an article in the blacklist,\n",
    "    drop the session. Currently, only the Main Page is\n",
    "    in the black list\n",
    "    \"\"\"\n",
    "\n",
    "    black_list = blacklist_pageids\n",
    "    for r in requests:\n",
    "        if r['page_id'] in black_list:\n",
    "            return False\n",
    "    return True\n",
    "   \n",
    "\n",
    "def sessionize(requests, dt = 3600):\n",
    "    \"\"\"\n",
    "    Break request stream whenever\n",
    "    there is a gap larger than dt [secs] in requests.\n",
    "    default is 3600s=1hour [from Halfaker et al. 2015]\n",
    "    \"\"\"\n",
    "    sessions = []\n",
    "    session = [requests[0]]\n",
    "    for r in requests[1:]:\n",
    "        d = r['ts'] -  session[-1]['ts']\n",
    "        if d > datetime.timedelta(seconds=dt):\n",
    "            sessions.append(session)\n",
    "            session = [r,]\n",
    "        else:\n",
    "            session.append(r)\n",
    "\n",
    "    sessions.append(session)\n",
    "    return sessions    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data multiple days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15580374]\n"
     ]
    }
   ],
   "source": [
    "# day_start = datetime.date(2020,4,1)\n",
    "# day_end = datetime.date(2020,5,1)\n",
    "# date_array = \\\n",
    "#     (day_start + datetime.timedelta(days=x) for x in range(0, (day_end-day_start).days))\n",
    "# wiki = 'simplewiki'\n",
    "\n",
    "day_start = datetime.date(2020,4,1)\n",
    "day_end = datetime.date(2020,4,8)\n",
    "date_array = \\\n",
    "    (day_start + datetime.timedelta(days=x) for x in range(0, (day_end-day_start).days))\n",
    "wiki = 'enwiki'\n",
    "\n",
    "\n",
    "blacklist_qids = ['Q5296',]\n",
    "blacklist_pageids = blacklist_qids_to_pageids(blacklist_qids,wiki)\n",
    "print(blacklist_pageids)\n",
    "dt = 3600\n",
    "nlen_min = 2\n",
    "nlen_max = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01\n",
      "2020-04-02\n",
      "2020-04-03\n",
      "2020-04-04\n",
      "2020-04-05\n",
      "2020-04-06\n",
      "2020-04-07\n"
     ]
    }
   ],
   "source": [
    "PATH_OUT = '/home/mgerlach/REPOS/reader-embedding/output/sessions/'\n",
    "for date_object in date_array:\n",
    "    day_str = date_object.strftime(\"%Y-%m-%d\")#str(datetime.date(year,month,day))\n",
    "    print(day_str)\n",
    "    filename= '/user/mgerlach/webrequest/sessions_filtered_%s_%s.parquet'%(wiki,day_str)\n",
    "    filename_save = 'sessions-filtered-pageid_%s_%s_dt%s_nmin%s_nmax%s'%(wiki,day_str,dt,nlen_min,nlen_max)\n",
    "\n",
    "    ## hdfs-storing\n",
    "    base_dir_hdfs = '/user/mgerlach/sessions'\n",
    "    output_hdfs_dir = os.path.join(base_dir_hdfs,filename_save)\n",
    "    os.system('hadoop fs -rm -r %s'%output_hdfs_dir)\n",
    "    ## local storing\n",
    "    base_dir_local =  PATH_OUT\n",
    "    output_local_dir_tmp = os.path.join(base_dir_local,'tmp',filename_save)\n",
    "    output_local_file = os.path.join(base_dir_local,filename_save)\n",
    "\n",
    "    ## load data\n",
    "    requests = spark.read.load(filename).rdd.map(lambda x: x['session'])\n",
    "    \n",
    "    requests = requests.map(lambda rs: [r for r in rs if r['page_id'] != None and r['project'] == wiki])\n",
    "    to_str = lambda x: ' '.join([str(e['page_id']) for e in x])\n",
    "\n",
    "    (requests\n",
    "     .map(parse_requests)\n",
    "     .filter(filter_blacklist_pageid) ## remove main_page\n",
    "     .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    "     .map(filter_consecutive_articles) ## remove consecutive calls to same article\n",
    "     .filter(lambda x: len(x)>nlen_min) ## only sessions with at least length nlen_min\n",
    "     .flatMap(lambda x: sessionize(x, dt = dt)) ## break sessions if interevent time is too large\n",
    "     .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    "     .filter(lambda x: len(x)<=nlen_max) ## only sessions with at most length nlen_max\n",
    "     .map(to_str) ## conctenate session as single string\n",
    "     ## write to hdfs\n",
    "     .saveAsTextFile(output_hdfs_dir,compressionCodecClass = \"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "    )\n",
    "\n",
    "    ## copy to local (set of tmp-dirs)\n",
    "    os.system('hadoop fs -copyToLocal %s %s'%(output_hdfs_dir,output_local_dir_tmp))\n",
    "    ## concatenate and unzip into single file\n",
    "    os.system('cat %s/* | gunzip > %s'%(output_local_dir_tmp,output_local_file))\n",
    "    ## remove set of tmp-dirs\n",
    "    os.system('rm -rf %s'%output_local_dir_tmp)\n",
    "    ## remove hadoop data\n",
    "    os.system('hadoop fs -rm -r %s'%output_hdfs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmfdata",
   "language": "python",
   "name": "venv_wmfdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

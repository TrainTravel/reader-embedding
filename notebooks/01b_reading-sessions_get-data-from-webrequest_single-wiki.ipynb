{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "get reading sessions for a day (all wikipedias)\n",
    "\n",
    "What are the filtering steps\n",
    "- all pageviews from wikipedias of a given day\n",
    "- filter bots by agent_type = user\n",
    "- filter app (only keep desktop, mobile web)\n",
    "- filter all sessions involving edit-attempt\n",
    "- filter all sessions with more than 100 pageviews (avoid bot-traffic)\n",
    "- only pageviews to main_namespace\n",
    "- join wikidata-items\n",
    "- aggregate to each line a session in the dataframe\n",
    "- save to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4048\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa2d5395a58>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_config = {}\n",
    "spark_config = {}\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.5'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark\n",
    "\n",
    "## regular\n",
    "\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"2g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 256\n",
    "# }\n",
    "# ## big\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"4g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 128,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 512\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEBREQuEST\n",
    "df_webreq = spark.read.table('wmf.webrequest')\n",
    "# df_webreq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01\n",
      "done in [s]: 758.05\n",
      "2020-04-02\n",
      "done in [s]: 693.12\n",
      "2020-04-03\n",
      "done in [s]: 680.85\n",
      "2020-04-04\n",
      "done in [s]: 717.71\n",
      "2020-04-05\n",
      "done in [s]: 927.17\n",
      "2020-04-06\n",
      "done in [s]: 1137.29\n",
      "2020-04-07\n",
      "done in [s]: 1076.73\n"
     ]
    }
   ],
   "source": [
    "## for several days\n",
    "day_start = datetime.date(2020,4,1)\n",
    "day_end = datetime.date(2020,4,8)\n",
    "\n",
    "date_array = \\\n",
    "    (day_start + datetime.timedelta(days=x) for x in range(0, (day_end-day_start).days))\n",
    "\n",
    "\n",
    "w = Window.partitionBy(F.col('user_hash'), F.col('year'), F.col('month'), F.col('day'))\n",
    "n_p_max = 100 \n",
    "n_p_min = 1\n",
    "wiki = 'enwiki'\n",
    "\n",
    "for date_object in date_array:\n",
    "    day_str = date_object.strftime(\"%Y-%m-%d\")#str(datetime.date(year,month,day))\n",
    "    year = date_object.year\n",
    "    month = date_object.month\n",
    "    day = date_object.day\n",
    "    print(day_str)\n",
    "    \n",
    "    ##\n",
    "    ## we hash the client-ip and the user-agent aka 'fingerprinting'\n",
    "    ## we add a salt\n",
    "    # salt for UA/IP hash\n",
    "    salt = ''.join(random.choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(random.randint(8,16)))\n",
    "    user_hash = F.sha2(F.concat(F.col('client_ip'),F.lit('-'),F.col('user_agent'),F.lit(salt)),512) ## only client and user\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    df_agg = (\n",
    "        df_webreq\n",
    "        .where( F.col('normalized_host.project') == wiki[:-4])\n",
    "        ## hash of user-id as new column\n",
    "        .withColumn('user_hash',user_hash)\n",
    "\n",
    "        ## select time partition    \n",
    "        .where( F.col('year')==year )\n",
    "        .where( F.col('month')==month )\n",
    "        .where( F.col('day')==day )\n",
    "#         .where( F.col('hour')==1 ) ## for testing reduce # partitions\n",
    "\n",
    "\n",
    "        ## select wiki project\n",
    "        .where( F.col('normalized_host.project_family') == \"wikipedia\" )\n",
    "\n",
    "        ## agent-type user to filter spiders\n",
    "        ## https://meta.wikimedia.org/wiki/Research:Page_view/Tags#Spider\n",
    "        .where(F.col('agent_type') == \"user\")\n",
    "\n",
    "        ## user: desktop/mobile/mobile app; isaac filters != mobile app\n",
    "        .where(F.col('access_method') != \"mobile app\")\n",
    "\n",
    "        ## not clear why; present in all cases I saw before.\n",
    "        .where(F.col('webrequest_source') ==  'text')\n",
    "\n",
    "\n",
    "        ## filter users who edited\n",
    "        .where( \n",
    "            (F.col('is_pageview') == 1)| \n",
    "            (F.col('uri_query').contains('action=edit')) | \n",
    "            (F.col('uri_query').contains('action=visualeditor')) | \n",
    "            (F.col('uri_query').contains('&intestactions=edit&intestactionsdetail=full&uiprop=options'))\n",
    "        )\n",
    "\n",
    "        ##### mark edit attempts (is_pageview==0)\n",
    "        .withColumn('edit_attempt', F.when(F.col('is_pageview')==0,1).otherwise(0) )\n",
    "        .withColumn('edit_attempt_session', F.max(F.col('edit_attempt')).over(w) )\n",
    "        .where(F.col('edit_attempt_session')==0)\n",
    "\n",
    "        ## only requests marked as pageviews\n",
    "        .where( F.col('is_pageview') == 1 )  \n",
    "\n",
    "        ## number of pageview requests per user and day between n_p_min and n_p_max\n",
    "        .withColumn('n_p_by_user', F.sum(F.col('is_pageview').cast(\"long\")).over(w) )\n",
    "        .where(F.col('n_p_by_user') >= n_p_min)\n",
    "        .where(F.col('n_p_by_user') <= n_p_max)    \n",
    "\n",
    "        ## only main namespace\n",
    "        .where( F.col('namespace_id') == 0 )\n",
    "                \n",
    "        .groupby('user_hash')\n",
    "        .agg(\n",
    "             F.first(F.col('access_method')).alias('access_method'),\n",
    "             F.first(F.col('geocoded_data')).alias('geocoded_data'),\n",
    "             F.first(F.col('n_p_by_user')).alias('session_length'),\n",
    "             F.array_sort(\n",
    "                 F.collect_list(\n",
    "                     F.struct(\n",
    "                         F.col('ts'),\n",
    "                         F.col('page_id'),\n",
    "                         F.col('pageview_info.page_title').alias('page_title'),\n",
    "                         F.concat(F.col('normalized_host.project'),F.lit('wiki')).alias('project'),\n",
    "                     )\n",
    "                 )\n",
    "             ).alias('session')\n",
    "         )\n",
    "    )\n",
    "\n",
    "\n",
    "    # clear salt so not accidentally retained\n",
    "    salt = None\n",
    "    filename_save = '/user/mgerlach/webrequest/sessions_filtered_%s_%s.parquet'%(wiki,day_str)\n",
    "    df_agg.write.mode('overwrite').parquet(filename_save)\n",
    "    t2 = time.time()\n",
    "    print('done in [s]: %.2f'%(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmfdata",
   "language": "python",
   "name": "venv_wmfdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

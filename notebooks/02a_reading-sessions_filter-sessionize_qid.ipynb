{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "filter sessions and write to txt-file on disc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import pandas as pd\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3cadc47978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_config = {}\n",
    "spark_config = {}\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.5'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark\n",
    "\n",
    "## regular\n",
    "\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"2g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 256\n",
    "# }\n",
    "# ## big\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"4g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 128,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 512\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining filter and maps\n",
    "def parse_requests(requests):\n",
    "    \"\"\"\n",
    "    do some initial parsing:\n",
    "    - drop pages without timestamp (we dont know which order)\n",
    "    \"\"\"\n",
    "    requests_clean = []\n",
    "    for r in requests:\n",
    "        if r['ts'] == None:\n",
    "            pass\n",
    "        else:\n",
    "            requests_clean += [r]\n",
    "    return requests_clean\n",
    "\n",
    "def filter_consecutive_articles(requests):\n",
    "    \"\"\"\n",
    "    Looking at the data, there are a lot of\n",
    "    sessions with the same article\n",
    "    requested 2 times in a row. This\n",
    "    does not make sense for training, so\n",
    "    lets collapse them into 1 request\n",
    "    \"\"\"\n",
    "    r = requests[0]\n",
    "    t = r['page_title']\n",
    "    clean_rs = [r,]\n",
    "    prev_t = t\n",
    "    for r in requests[1:]:\n",
    "        t = r['page_title']\n",
    "        if t == prev_t:\n",
    "            continue\n",
    "        else:\n",
    "            clean_rs.append(r)\n",
    "            prev_t = t\n",
    "    return clean_rs\n",
    "\n",
    "def filter_blacklist_qid(requests):\n",
    "    \"\"\"\n",
    "    If the session contains an article in the blacklist,\n",
    "    drop the session. Currently, only the Main Page is\n",
    "    in the black list\n",
    "    \"\"\"\n",
    "\n",
    "    black_list = set(['Q5296',])\n",
    "    for r in requests:\n",
    "        if r['qid'] in black_list:\n",
    "            return False\n",
    "    return True\n",
    "   \n",
    "\n",
    "def sessionize(requests, dt = 3600):\n",
    "    \"\"\"\n",
    "    Break request stream whenever\n",
    "    there is a gap larger than dt [secs] in requests.\n",
    "    default is 3600s=1hour [from Halfaker et al. 2015]\n",
    "    \"\"\"\n",
    "    sessions = []\n",
    "    session = [requests[0]]\n",
    "    for r in requests[1:]:\n",
    "        d = r['ts'] -  session[-1]['ts']\n",
    "        if d > datetime.timedelta(seconds=dt):\n",
    "            sessions.append(session)\n",
    "            session = [r,]\n",
    "        else:\n",
    "            session.append(r)\n",
    "\n",
    "    sessions.append(session)\n",
    "    return sessions    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data multiple days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-27\n",
      "2020-04-28\n",
      "2020-04-29\n",
      "2020-04-30\n",
      "2020-05-01\n",
      "2020-05-02\n",
      "2020-05-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f6a80fdd8483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m      \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## conctenate session as single string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m      \u001b[0;31m## write to hdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m      \u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_hdfs_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompressionCodecClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"org.apache.hadoop.io.compress.GzipCodec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompressionCodecClass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mcompressionCodec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressionCodecClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "day_start = datetime.date(2020,4,27)\n",
    "day_end = datetime.date(2020,5,4)\n",
    "date_array = \\\n",
    "    (day_start + datetime.timedelta(days=x) for x in range(0, (day_end-day_start).days))\n",
    "\n",
    "dt = 3600\n",
    "nlen_min = 2\n",
    "nlen_max = 30\n",
    "lang = 'simplewiki'\n",
    "# lang = 'wikidata'\n",
    "\n",
    "\n",
    "PATH_OUT = '/home/mgerlach/REPOS/reader-embedding/output/sessions/'\n",
    "for date_object in date_array:\n",
    "    day_str = date_object.strftime(\"%Y-%m-%d\")#str(datetime.date(year,month,day))\n",
    "    print(day_str)\n",
    "    filename= '/user/mgerlach/webrequest/sessions_filtered_qids_%s.parquet'%(day_str)\n",
    "    filename_save = 'sessions-filtered-qid_%s_%s_dt%s_nmin%s_nmax%s'%(lang,day_str,dt,nlen_min,nlen_max)\n",
    "\n",
    "    ## hdfs-storing\n",
    "    base_dir_hdfs = '/user/mgerlach/sessions'\n",
    "    output_hdfs_dir = os.path.join(base_dir_hdfs,filename_save)\n",
    "    os.system('hadoop fs -rm -r %s'%output_hdfs_dir)\n",
    "    ## local storing\n",
    "    base_dir_local =  PATH_OUT\n",
    "    output_local_dir_tmp = os.path.join(base_dir_local,'tmp',filename_save)\n",
    "    output_local_file = os.path.join(base_dir_local,filename_save)\n",
    "\n",
    "    ## load data\n",
    "    requests = spark.read.load(filename).rdd.map(lambda x: x['session'])\n",
    "    \n",
    "    ## keep only pageviews from a language\n",
    "    if lang == 'wikidata':\n",
    "        requests = requests.map(lambda rs: [r for r in rs if r['qid'] != None])\n",
    "        to_str = lambda x: ' '.join([str(e['qid']) for e in x])\n",
    "    else:\n",
    "        requests = requests.map(lambda rs: [r for r in rs if r['qid'] != None and r['project'] == lang])\n",
    "        to_str = lambda x: ' '.join([str(e['qid']) for e in x])\n",
    "\n",
    "    (requests\n",
    "     .map(parse_requests)\n",
    "     .filter(filter_blacklist_qid) ## remove main_page\n",
    "     .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    "     .map(filter_consecutive_articles) ## remove consecutive calls to same article\n",
    "     .filter(lambda x: len(x)>nlen_min) ## only sessions with at least length nlen_min\n",
    "     .flatMap(lambda x: sessionize(x, dt = dt)) ## break sessions if interevent time is too large\n",
    "     .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    "     .filter(lambda x: len(x)<=nlen_max) ## only sessions with at most length nlen_max\n",
    "     .map(to_str) ## conctenate session as single string\n",
    "     ## write to hdfs\n",
    "     .saveAsTextFile(output_hdfs_dir,compressionCodecClass = \"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "    )\n",
    "\n",
    "    ## copy to local (set of tmp-dirs)\n",
    "    os.system('hadoop fs -copyToLocal %s %s'%(output_hdfs_dir,output_local_dir_tmp))\n",
    "    ## concatenate and unzip into single file\n",
    "    os.system('cat %s/* | gunzip > %s'%(output_local_dir_tmp,output_local_file))\n",
    "    ## remove set of tmp-dirs\n",
    "    os.system('rm -rf %s'%output_local_dir_tmp)\n",
    "    ## remove hadoop data\n",
    "    os.system('hadoop fs -rm -r %s'%output_hdfs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmfdata",
   "language": "python",
   "name": "venv_wmfdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

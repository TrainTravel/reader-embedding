{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "filter and sessionize the reading-sessions extracted from webrequest\n",
    "- map to qid or pageid and remove pageviews that do not have an id\n",
    "    - if lang=='wikidata': keep pageviews from all projects which have a qid\n",
    "    - if lang=='*wiki' (e.g. enwiki): keep pageviews from a single project which have a pageid.\n",
    "- keep pageview only if it has a timestamp\n",
    "- collapse pageviews if same page was viewed consecutively\n",
    "- remove sessions which contain the main-page\n",
    "- split sessions of interevent time is larger than 1 hour\n",
    "- keep only sessions with >= 2 and <= 30 pageviews\n",
    "\n",
    "Output is a file on disk \n",
    "    - PATH_OUT/reading-sessions-filtered_<DAY>_<LANG>\n",
    "        - <DAY> = 2020-05-01\n",
    "        - <LANG> = wikidata, enwiki, dewiki, ...\n",
    "    - this is a textfile where each line is a session and pageviews in the session are separated by whitespace\n",
    "        - if <LANG> == 'wikidata' a succession of qids: e.g. Q123 Q2347523 Q23452354 \\n\n",
    "        - if <LANG> == '*wiki' a successoin of pageids: 212375 123621 12 123 \\n\n",
    "    \n",
    "As input we need the parquet-files generated in reading-sessions_01-get-data-from-webrequest.\n",
    "    \n",
    "    \n",
    "Required packages:\n",
    "    - findspark\n",
    "    - wmfdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata v1.0.1, but v1.0.2 is available.\n",
      "\n",
      "To update, run `pip install --upgrade git+https://github.com/neilpquinn/wmfdata/wmfdata.git@release`.\n",
      "\n",
      "To see the changes, refer to https://github.com/neilpquinn/wmfdata/blob/release/CHANGELOG.md\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f691a90f8d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_config = {}\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"2g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 256\n",
    "# }\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.5'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining filter and maps\n",
    "def parse_requests(requests):\n",
    "    \"\"\"\n",
    "    do some initial parsing:\n",
    "    - drop pages without timestamp (we dont know which order)\n",
    "    \"\"\"\n",
    "    requests_clean = []\n",
    "    for r in requests:\n",
    "        if r['ts'] == None:\n",
    "            pass\n",
    "        else:\n",
    "            requests_clean += [r]\n",
    "    return requests_clean\n",
    "\n",
    "def filter_consecutive_articles(requests):\n",
    "    \"\"\"\n",
    "    Looking at the data, there are a lot of\n",
    "    sessions with the same article\n",
    "    requested 2 times in a row. This\n",
    "    does not make sense for training, so\n",
    "    lets collapse them into 1 request.\n",
    "    We compare page-ids and not:\n",
    "    - page-titles due to redirects (page-title keeps the redirect-from, page-id resolves redirect)\n",
    "    - qids to capture possible language switching.\n",
    "    \"\"\"\n",
    "    r = requests[0]\n",
    "    t = r['page_id']\n",
    "    clean_rs = [r,]\n",
    "    prev_t = t\n",
    "    for r in requests[1:]:\n",
    "        t = r['page_id']\n",
    "        if t == prev_t:\n",
    "            continue\n",
    "        else:\n",
    "            clean_rs.append(r)\n",
    "            prev_t = t\n",
    "    return clean_rs\n",
    "\n",
    "def filter_blacklist_qid(requests):\n",
    "    \"\"\"\n",
    "    If the session contains an article in the blacklist,\n",
    "    drop the session. Currently, only the Main Page is\n",
    "    in the black list\n",
    "    \"\"\"\n",
    "\n",
    "    black_list = set(['Q5296',])\n",
    "    for r in requests:\n",
    "        if r['qid'] in black_list:\n",
    "            return False\n",
    "    return True\n",
    "   \n",
    "\n",
    "def sessionize(requests, dt = 3600):\n",
    "    \"\"\"\n",
    "    Break request stream whenever\n",
    "    there is a gap larger than dt [secs] in requests.\n",
    "    default is 3600s=1hour [from Halfaker et al. 2015]\n",
    "    \"\"\"\n",
    "    sessions = []\n",
    "    session = [requests[0]]\n",
    "    for r in requests[1:]:\n",
    "        d = r['ts'] -  session[-1]['ts']\n",
    "        if d > datetime.timedelta(seconds=dt):\n",
    "            sessions.append(session)\n",
    "            session = [r,]\n",
    "        else:\n",
    "            session.append(r)\n",
    "\n",
    "    sessions.append(session)\n",
    "    return sessions    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data multiple days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01\n",
      "done in [s]: 168.04\n",
      "2020-04-02\n",
      "done in [s]: 168.87\n",
      "2020-04-03\n",
      "done in [s]: 154.69\n",
      "2020-04-04\n",
      "done in [s]: 145.23\n",
      "2020-04-05\n",
      "done in [s]: 158.93\n",
      "2020-04-06\n",
      "done in [s]: 154.14\n",
      "2020-04-07\n",
      "done in [s]: 133.10\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "## select time interval (daily)\n",
    "day_start = datetime.date(2020,4,1)\n",
    "day_end = datetime.date(2020,4,8)\n",
    "date_array = \\\n",
    "    (day_start + datetime.timedelta(days=x) for x in range(0, (day_end-day_start).days))\n",
    "\n",
    "## select language\n",
    "# lang = 'wikidata' ## wikidata aggregates all wiki (qids)\n",
    "lang = 'enwiki' ## individual language keeps only one project (pageids)\n",
    "\n",
    "\n",
    "\n",
    "dt = 3600 ## cutoff for splitting sessions(interevent time between 2 pageivews)\n",
    "nlen_min = 2 ## min length of session\n",
    "nlen_max = 30 ## max length of session\n",
    "\n",
    "\n",
    "\n",
    "## folder to the parquet-files from the extracted webrequest data\n",
    "PATH_IN = '/user/mgerlach/webrequest/' \n",
    "## folder where to save the filtered reading sesssions\n",
    "PATH_OUT = '/home/mgerlach/REPOS/reader-embedding/output/reading-sessions-filtered/'\n",
    "\n",
    "\n",
    "for date_object in date_array:\n",
    "    t1 = time.time()\n",
    "    day_str = date_object.strftime(\"%Y-%m-%d\")#str(datetime.date(year,month,day))\n",
    "    print(lang, day_str)\n",
    "    filename=os.path.join(PATH_IN,'reading-sessions-webrequest_%s.parquet'%(day_str))\n",
    "    filename_save = 'reading-sessions-filtered_%s_%s'%(lang,day_str)\n",
    "\n",
    "    ## hdfs-storing, some temporary files which will be deleted later\n",
    "    base_dir_hdfs = '/user/mgerlach/sessions'\n",
    "    output_hdfs_dir = os.path.join(base_dir_hdfs,filename_save)\n",
    "    os.system('hadoop fs -rm -r %s'%output_hdfs_dir)\n",
    "    ## local storing\n",
    "    base_dir_local =  PATH_OUT\n",
    "    output_local_dir_tmp = os.path.join(base_dir_local,'tmp',filename_save)\n",
    "    output_local_file = os.path.join(base_dir_local,filename_save)\n",
    "\n",
    "    ## load data\n",
    "    requests = spark.read.load(filename).rdd.map(lambda x: x['session'])\n",
    "    \n",
    "    ## keep only pageviews from a language\n",
    "    if lang == 'wikidata':\n",
    "        requests = requests.map(lambda rs: [r for r in rs if r['qid'] != None])\n",
    "        to_str = lambda x: ' '.join([str(e['qid']) for e in x])\n",
    "    else:\n",
    "        requests = requests.map(lambda rs: [r for r in rs if r['page_id'] != None and r['project'] == lang])\n",
    "        to_str = lambda x: ' '.join([str(e['page_id']) for e in x])\n",
    "\n",
    "    (requests\n",
    "     .map(parse_requests)\n",
    "     .filter(filter_blacklist_qid) ## remove main_page\n",
    "     .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    "     .map(filter_consecutive_articles) ## remove consecutive calls to same article\n",
    "     .filter(lambda x: len(x)>nlen_min) ## only sessions with at least length nlen_min\n",
    "     .flatMap(lambda x: sessionize(x, dt = dt)) ## break sessions if interevent time is too large\n",
    "     .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    "     .filter(lambda x: len(x)<=nlen_max) ## only sessions with at most length nlen_max\n",
    "     .map(to_str) ## conctenate session as single string\n",
    "     ## write to hdfs\n",
    "     .saveAsTextFile(output_hdfs_dir,compressionCodecClass = \"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "    )\n",
    "\n",
    "    ## copy to local (set of tmp-dirs)\n",
    "    os.system('hadoop fs -copyToLocal %s %s'%(output_hdfs_dir,output_local_dir_tmp))\n",
    "    ## concatenate and unzip into single file\n",
    "    os.system('cat %s/* | gunzip > %s'%(output_local_dir_tmp,output_local_file))\n",
    "    ## remove set of tmp-dirs\n",
    "    os.system('rm -rf %s'%output_local_dir_tmp)\n",
    "    ## remove hadoop data\n",
    "    os.system('hadoop fs -rm -r %s'%output_hdfs_dir)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('done in [s]: %.2f'%(t2-t1))\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmfdata",
   "language": "python",
   "name": "venv_wmfdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

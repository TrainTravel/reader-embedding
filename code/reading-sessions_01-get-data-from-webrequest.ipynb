{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "get reading sessions for a day (all wikipedias)\n",
    "\n",
    "What are the filtering steps\n",
    "- all pageviews from wikipedias of a given day\n",
    "- filter bots by agent_type = user\n",
    "- filter app (only keep desktop, mobile web)\n",
    "- filter all sessions involving edit-attempt\n",
    "- filter all sessions with more than 100 pageviews (avoid bot-traffic)\n",
    "- only pageviews to main_namespace\n",
    "- join wikidata-items\n",
    "- aggregate to each line a session in the dataframe\n",
    "- save to parquet\n",
    "\n",
    "Output is one parquet-file for each day in\n",
    "    - PATH_OUT/reading-sessions-webrequest_<DAY>.parquet\n",
    "        - <DAY> = '2020-05-03'\n",
    "    \n",
    "    \n",
    "Required packages:\n",
    "    - findspark\n",
    "    - wmfdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata v1.0.1, but v1.0.2 is available.\n",
      "\n",
      "To update, run `pip install --upgrade git+https://github.com/neilpquinn/wmfdata/wmfdata.git@release`.\n",
      "\n",
      "To see the changes, refer to https://github.com/neilpquinn/wmfdata/blob/release/CHANGELOG.md\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8109b54470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## defining the spark session\n",
    "spark_config = {}\n",
    "## regular\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"2g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 256\n",
    "# }\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.5'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is where the parquet files will be saved\n",
    "PATH_OUT = '/user/mgerlach/webrequest/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## WIKIDATA: join qids\n",
    "##### wikidata-pageid table to join in the wikidata ids\n",
    "##### https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Wikidata_item_page_link\n",
    "##### see available snapshots via hadoop fs -ls /wmf/data/wmf/wikidata/item_page_link/\n",
    "##### looking at all previous snapshots we keep all unique combinations of qid | project+page_id\n",
    "\n",
    "\n",
    "wd_wiki_pageid = F.concat(F.col('wiki_db'),F.lit('-'),F.col('page_id'))\n",
    "df_wd = (\n",
    "    spark.read.table('wmf.wikidata_item_page_link')\n",
    "    ## snapshot: this is a partition!\n",
    "    .where(F.col('snapshot') >= '2020-01-06') ## resolve issues with non-mathcing wikidata-items\n",
    "    ## only wikis (enwiki, ... not: wikisource)\n",
    "    .where(F.col('wiki_db').endswith('wiki'))\n",
    "    .withColumn('wiki_pageid',wd_wiki_pageid)\n",
    "    .select(\n",
    "        'item_id',\n",
    "        'wiki_pageid',\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")\n",
    "# df_wd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-07\n",
      "done in [s]: 1052.41\n",
      "2020-04-08\n",
      "done in [s]: 1662.77\n",
      "2020-04-09\n",
      "done in [s]: 1440.22\n",
      "2020-04-10\n",
      "done in [s]: 1352.41\n",
      "2020-04-11\n",
      "done in [s]: 1500.09\n",
      "2020-04-12\n",
      "done in [s]: 1476.39\n",
      "2020-04-13\n"
     ]
    }
   ],
   "source": [
    "## for several days\n",
    "day_start = datetime.date(2020,4,7) ## starting date\n",
    "day_end = datetime.date(2020,5,1) ## end-date (non-inclusive)\n",
    "\n",
    "date_array = \\\n",
    "    (day_start + datetime.timedelta(days=x) for x in range(0, (day_end-day_start).days))\n",
    "\n",
    "## partition for max/min number of pageviews per day\n",
    "w = Window.partitionBy(F.col('user_hash'), F.col('year'), F.col('month'), F.col('day'))\n",
    "n_p_max = 100 ## maximum number of pageviews/user/day\n",
    "n_p_min = 1 ## minimum number of pageviews/user/day\n",
    "\n",
    "## join project and page-id\n",
    "webrequest_wiki_pageid = F.concat(F.col('normalized_host.project'),F.lit('wiki-'),F.col('page_id'))\n",
    "\n",
    "bool_reduced = False ## for testing\n",
    "\n",
    "## WEBREQuEST data\n",
    "df_webreq = spark.read.table('wmf.webrequest')\n",
    "\n",
    "for date_object in date_array:\n",
    "    day_str = date_object.strftime(\"%Y-%m-%d\")#str(datetime.date(year,month,day))\n",
    "    year = date_object.year\n",
    "    month = date_object.month\n",
    "    day = date_object.day\n",
    "    print(day_str)\n",
    "    \n",
    "    ##\n",
    "    ## we hash the client-ip and the user-agent aka 'fingerprinting'\n",
    "    ## we add a different salt for each day\n",
    "    salt = ''.join(random.choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(random.randint(8,16)))\n",
    "    user_hash = F.sha2(F.concat(F.col('client_ip'),F.lit('-'),F.col('user_agent'),F.lit(salt)),512) ## only client and user\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    if bool_reduced == True:\n",
    "        df_agg = (\n",
    "            df_webreq\n",
    "            .where( F.col('hour')==1 )\n",
    "            .where( F.col('normalized_host.project') == \"simple\" )\n",
    "        )\n",
    "    else:\n",
    "        df_agg = df_webreq\n",
    "\n",
    "\n",
    "    df_agg = (\n",
    "        df_agg\n",
    "        ## hash of user-id as new column\n",
    "        .withColumn('user_hash',user_hash)\n",
    "\n",
    "        ## select time partition    \n",
    "        .where( F.col('year')==year )\n",
    "        .where( F.col('month')==month )\n",
    "        .where( F.col('day')==day )\n",
    "    #     .where( F.col('hour')==hour )\n",
    "\n",
    "\n",
    "        ## select wiki project\n",
    "        .where( F.col('normalized_host.project_family') == \"wikipedia\" )\n",
    "\n",
    "        ## agent-type user to filter spiders\n",
    "        ## https://meta.wikimedia.org/wiki/Research:Page_view/Tags#Spider\n",
    "        .where(F.col('agent_type') == \"user\")\n",
    "\n",
    "        ## user: desktop/mobile/mobile app; isaac filters != mobile app\n",
    "        .where(F.col('access_method') != \"mobile app\")\n",
    "\n",
    "        ## not clear why; present in all cases I saw before.\n",
    "        .where(F.col('webrequest_source') ==  'text')\n",
    "\n",
    "\n",
    "        ## filter users who edited\n",
    "        .where( \n",
    "            (F.col('is_pageview') == 1)| \n",
    "            (F.col('uri_query').contains('action=edit')) | \n",
    "            (F.col('uri_query').contains('action=visualeditor')) | \n",
    "            (F.col('uri_query').contains('&intestactions=edit&intestactionsdetail=full&uiprop=options'))\n",
    "        )\n",
    "\n",
    "        ##### mark edit attempts (is_pageview==0)\n",
    "        .withColumn('edit_attempt', F.when(F.col('is_pageview')==0,1).otherwise(0) )\n",
    "        .withColumn('edit_attempt_session', F.max(F.col('edit_attempt')).over(w) )\n",
    "        .where(F.col('edit_attempt_session')==0)\n",
    "\n",
    "        ## only requests marked as pageviews\n",
    "        .where( F.col('is_pageview') == 1 )  \n",
    "\n",
    "        ## number of pageview requests per user and day between n_p_min and n_p_max\n",
    "        .withColumn('n_p_by_user', F.sum(F.col('is_pageview').cast(\"long\")).over(w) )\n",
    "        .where(F.col('n_p_by_user') >= n_p_min)\n",
    "        .where(F.col('n_p_by_user') <= n_p_max)    \n",
    "\n",
    "        ## only main namespace\n",
    "        .where( F.col('namespace_id') == 0 )\n",
    "\n",
    "        ## merging the wikdidata id\n",
    "        #### new column enwiki-234232\n",
    "        .withColumn('wiki_pageid', webrequest_wiki_pageid )\n",
    "        .join(df_wd,on='wiki_pageid',how='left_outer')\n",
    "        \n",
    "        .groupby('user_hash')\n",
    "        .agg(\n",
    "             F.first(F.col('access_method')).alias('access_method'),\n",
    "             F.first(F.col('geocoded_data')).alias('geocoded_data'),\n",
    "             F.first(F.col('n_p_by_user')).alias('session_length'),\n",
    "             F.array_sort(\n",
    "                 F.collect_list(\n",
    "                     F.struct(\n",
    "                         F.col('ts'),\n",
    "                         F.col('page_id'),\n",
    "                         F.col('pageview_info.page_title').alias('page_title'),\n",
    "                         F.concat(F.col('normalized_host.project'),F.lit('wiki')).alias('project'),\n",
    "                         F.col('item_id').alias('qid'),\n",
    "                     )\n",
    "                 )\n",
    "             ).alias('session')\n",
    "         )\n",
    "    )\n",
    "\n",
    "\n",
    "    # clear salt so not accidentally retained\n",
    "    salt = None\n",
    "    if bool_reduced == True:\n",
    "        filename_save = os.path.join(PATH_OUT,'reading-sessions-webrequest_reduced_%s.parquet'%(day_str))\n",
    "    else:\n",
    "        filename_save = os.path.join(PATH_OUT,'reading-sessions-webrequest_%s.parquet'%(day_str))\n",
    "    df_agg.write.mode('overwrite').parquet(filename_save)\n",
    "    t2 = time.time()\n",
    "    print('done in [s]: %.2f'%(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmfdata",
   "language": "python",
   "name": "venv_wmfdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

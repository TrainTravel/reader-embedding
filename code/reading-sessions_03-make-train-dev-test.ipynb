{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Make training and dev and test datasets from a set of files containing reading sessions\n",
    "\n",
    "\n",
    "Required packages:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import os,sys\n",
    "import json\n",
    "import time\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files:  1\n"
     ]
    }
   ],
   "source": [
    "## select time period\n",
    "day_start = datetime.date(2020,4,1)\n",
    "day_end = datetime.date(2020,4,8)\n",
    "date_array = \\\n",
    "    (day_start + datetime.timedelta(days=x) for x in range(0, (day_end-day_start).days))\n",
    "\n",
    "## select language\n",
    "# lang = 'wikidata'\n",
    "lang = 'enwiki'\n",
    "\n",
    "## select size of subsample (for wikidata there are ~50M sessions/day which might be too much), \n",
    "mult = 100000 ## in multiples of 100k\n",
    "list_N_size = [1,2,1000,-1] ## -1 indicates the full dataset\n",
    "\n",
    "## split into train-dev-test\n",
    "p_train, p_dev,p_test = 0.8,0.1,0.1 ## has to sum to 1\n",
    "\n",
    "\n",
    "PATH_IN = '/home/mgerlach/REPOS/reader-embedding/output/reading-sessions-filtered/'\n",
    "PATH_OUT = '/home/mgerlach/REPOS/reader-embedding/output/reading-sessions-corpora/'\n",
    "## get all files from reading sessions\n",
    "list_filenames = []\n",
    "for date_object in date_array:\n",
    "    day_str = date_object.strftime(\"%Y-%m-%d\")#str(datetime.date(year,month,day))\n",
    "    filename = os.path.join(\n",
    "        PATH_IN,\n",
    "       'reading-sessions-filtered_%s_%s'%(lang,day_str)\n",
    "    )\n",
    "    if os.path.isfile(filename):\n",
    "        list_filenames += [filename]\n",
    "print('Number of files: ',len(list_filenames))\n",
    "\n",
    "\n",
    "## use multiple cores to read different files (at most 10)\n",
    "n_cores = min(10,len(list_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get total number of sessions in all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sessions:  24150756\n"
     ]
    }
   ],
   "source": [
    "## get the number of sessions across all datasets\n",
    "def read_sessions_from_file(filename_in):\n",
    "    N_kept = 0\n",
    "    for line in open(filename_in):\n",
    "        N_kept+=1\n",
    "    return N_kept\n",
    "\n",
    "try:\n",
    "    pool = Pool(n_cores)\n",
    "    list_N_kept = pool.map(read_sessions_from_file,list_filenames)\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "## number of total lines\n",
    "N=sum(list_N_kept)\n",
    "print('Total number of sessions: ',N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create randomly sampled corpora of different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sessions_from_file(filename_in):\n",
    "    with open(filename_tmp,'a') as fout:\n",
    "        for line in open(filename_in):\n",
    "            write_line = True\n",
    "            ## write the line with some sampling probability\n",
    "            if p_sample<1.:\n",
    "                if random.random()>p_sample:\n",
    "                    write_line=False\n",
    "            if write_line==True:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 100000\n",
      "2 200000\n",
      "-1 24150756\n"
     ]
    }
   ],
   "source": [
    "for N_size in list_N_size:\n",
    "    if N_size<0:\n",
    "        N_sel = N\n",
    "    elif N_size*mult<=N:\n",
    "        N_sel = N_size*mult\n",
    "    else:\n",
    "        continue\n",
    "    print(N_size, N_sel)\n",
    "\n",
    "        \n",
    "    p_sample = min([1.1*N_sel/N,1.])\n",
    "    filename_tmp = os.path.join(PATH_OUT,'tmp.corpus')\n",
    "    with open(filename_tmp,'w') as fout:\n",
    "        fout.write('')\n",
    "    try:\n",
    "        pool = Pool(n_cores)\n",
    "        pool.map(sample_sessions_from_file,list_filenames)\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "    \n",
    "    filename_base = os.path.join(PATH_OUT,'%s_sample-%s'%(lang,N_sel))\n",
    "    ## make sure we only have N_sel samples and write to corpus-file\n",
    "    filename_out_corpus = filename_base+'.corpus'\n",
    "    with open(filename_out_corpus,'w') as fout:\n",
    "        i=0\n",
    "        for line in open(filename_tmp):\n",
    "            fout.write(line)\n",
    "            i+=1\n",
    "            if i==N_sel:\n",
    "                break\n",
    "                                      \n",
    "    ## split into train-dev-test          \n",
    "    filename_out_train = filename_base+'.train'\n",
    "    filename_out_dev = filename_base+'.dev'\n",
    "    filename_out_test = filename_base+'.test'\n",
    "\n",
    "    N_train = int(N_sel*p_train)\n",
    "    N_dev = int(N_sel*p_dev)\n",
    "    i=0\n",
    "    \n",
    "    with open(filename_out_train,'w') as fout_train, open(filename_out_dev,'w') as fout_dev, open(filename_out_test,'w') as fout_test:\n",
    "        for line in open(filename_out_corpus):\n",
    "            if i<N_train:\n",
    "                fout_train.write(line)\n",
    "            elif i>=N_train and i<N_train+N_dev:\n",
    "                fout_dev.write(line)\n",
    "            elif i>=N_train+N_dev:\n",
    "                fout_test.write(line)\n",
    "            else:\n",
    "                pass\n",
    "            i+=1\n",
    "                                 \n",
    "    ## delete the tmp-file          \n",
    "    os.system('rm -r %s'%filename_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gensim",
   "language": "python",
   "name": "venv_gensim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

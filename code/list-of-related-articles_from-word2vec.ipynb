{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import time\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import json\n",
    "import requests\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "day_start = datetime.date(2020,4,27)\n",
    "day_end = datetime.date(2020,4,28)\n",
    "lang = 'wikidata'\n",
    "\n",
    "mode = 'cbow' ## (if 1: skip-gram, else cbow)\n",
    "size = 50 ## number of dimensions\n",
    "window = 5 ## context window size\n",
    "sample = 0.001 ## downsample high-frequency words\n",
    "negative = 5 ##negative sampling (noise words)\n",
    "min_count = 20 ## words with less occurrences in total will be ignored\n",
    "epochs  = 5 ## number of iterations\n",
    "\n",
    "PATH_file = '../output/models/'\n",
    "# # ## one week\n",
    "filename = os.path.join(\n",
    "    PATH_file,\n",
    "    'word2vec_%s_%s--%s_params-%s-%s-%s-%s-%s-%s-%s.bin'%(\n",
    "        lang,str(day_start),str(day_end),\n",
    "        mode,size,window,sample,negative,min_count,epochs\n",
    "    )\n",
    ")\n",
    "model = fasttext.load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set1: xenophobia\n",
    "# list_qid_seed = ['Q84318312']\n",
    "# N_nn = 200 ## initial number of nearet neighbors\n",
    "# list_wikis = ['enwiki']\n",
    "# list_keywords = []## if labels contain these words, remove item\n",
    "# N_max=100 ## how many pages to keep from candidates\n",
    "# formatted = False ## if formatting for wikitext\n",
    "\n",
    "\n",
    "## set2: pandemic, disease\n",
    "list_qid_seed = ['Q81068910','Q84263196']\n",
    "N_nn = 1000\n",
    "list_wikis = ['enwiki','dewiki','frwiki','eswiki','ruwiki','zhwiki','ptwiki','arwiki','bnwiki','hiwiki']\n",
    "list_keywords = ['covid','corona','cov'] ## if labels contain these words, remove item\n",
    "N_max = 200\n",
    "formatted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get nearest neighbors of seed-article and query titles/sitelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getItems(list_qids,list_wikis=None, n_batch=30):\n",
    "    '''\n",
    "    for list of qids, get:\n",
    "    - label\n",
    "    - description\n",
    "    - titles (sitelinks)\n",
    "    if list_wikis is specified, only get titles for specific wikis (e.g. enwiki)\n",
    "    \n",
    "    process the qids in batches of n_batches (at most 50, but make smaller to be sure).\n",
    "    use: https://www.mediawiki.org/wiki/Wikibase/API\n",
    "    '''\n",
    "    api_url_base = 'https://wikidata.org/w/api.php'\n",
    "    ## split into batches\n",
    "    list_qids_split = np.array_split(list_qids,math.ceil(len(list_qids)/n_batch))\n",
    "    list_items = []\n",
    "    i_qid=0    \n",
    "    for list_qids_batch in list_qids_split:\n",
    "        \n",
    "        params = {\n",
    "            'action':'wbgetentities',\n",
    "            'props':'sitelinks|labels|descriptions',\n",
    "            'languages':'en',\n",
    "            'format' : 'json',\n",
    "            'ids':'|'.join(list_qids_batch),\n",
    "        }\n",
    "        if list_wikis != None:\n",
    "            params['sitefilter']:'|'.join(list_wikis)\n",
    "\n",
    "        response = requests.get( api_url_base,params=params)\n",
    "        result=json.loads(response.text)\n",
    "        \n",
    "        for qid in list_qids_batch:\n",
    "            dict_item = {'qid':qid}\n",
    "\n",
    "            ## titles via sitelinks\n",
    "            dict_title = {} ## collect all titles of an item via sitelinks\n",
    "            titles = result['entities'].get(qid,{}).get('sitelinks',{})\n",
    "            for wiki, wiki_title in titles.items():\n",
    "                title = wiki_title['title'].replace(' ','_')\n",
    "                dict_title[wiki] = title\n",
    "            dict_item['titles'] = dict_title\n",
    "            \n",
    "            ## labels: put '-' if not available (so far only en)\n",
    "            label = result['entities'].get(qid,{}).get('labels',{}).get('en',{}).get('value','-')\n",
    "            dict_item['label'] = label\n",
    "            \n",
    "            ## description: put '-' if not available (so far only en)\n",
    "            description = result['entities'].get(qid,{}).get('descriptions',{}).get('en',{}).get('value','-')\n",
    "            dict_item['description'] = description\n",
    "            \n",
    "            list_items += [dict_item]\n",
    "            \n",
    "            i_qid+=1    \n",
    "    return list_items\n",
    "            \n",
    "\n",
    "def items_add_titles(list_items, list_wikis = ['enwiki']):\n",
    "    '''\n",
    "    \n",
    "    add information about a list of qids:\n",
    "    - label\n",
    "    - description\n",
    "    - article-titles (sitelinks) for a set of wikis\n",
    "    '''\n",
    "    \n",
    "    list_qids = [h['qid'] for h in list_items]\n",
    "    list_items_meta = getItems(list_qids, list_wikis=list_wikis)\n",
    "    \n",
    "    list_items_new = list_items\n",
    "    for i_item, item in enumerate(list_items):\n",
    "        dict_titles = {}\n",
    "        for wiki in list_wikis:\n",
    "            title =  list_items_meta[i_item]['titles'].get(wiki,'-')\n",
    "            dict_titles[wiki] = title\n",
    "        list_items_new[i_item]['titles'] = dict_titles\n",
    "        list_items_new[i_item]['label'] = list_items_meta[i_item]['label']\n",
    "        list_items_new[i_item]['description'] = list_items_meta[i_item]['description']\n",
    "        \n",
    "        ## add text\n",
    "        text = list_items_new[i_item]['label']\n",
    "        if text == '-':\n",
    "            text_tmp =  list_items_new[i_item]['description']\n",
    "            if text_tmp!='-':\n",
    "                text = text_tmp          \n",
    "\n",
    "        for wiki in list_wikis:\n",
    "            if text != '-':\n",
    "                break\n",
    "            else:\n",
    "                text_tmp = list_items_new[i_item]['titles'][wiki]\n",
    "                if text_tmp!='-':\n",
    "                    text = text_tmp+' [title-%s]'%(wiki)\n",
    "        list_items_new[i_item]['text'] = text\n",
    "    return list_items_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q82069695', 'score': 0.8901513814926147, 'titles': {'enwiki': 'Severe_acute_respiratory_syndrome_coronavirus_2', 'dewiki': 'SARS-CoV-2', 'frwiki': 'Coronavirus_2_du_syndrome_respiratoire_aigu_sévère', 'eswiki': 'SARS-CoV-2', 'ruwiki': 'SARS-CoV-2', 'zhwiki': '严重急性呼吸系统综合征冠状病毒2', 'ptwiki': 'Coronavírus_da_síndrome_respiratória_aguda_grave_2', 'arwiki': 'فيروس_كورونا_المرتبط_بالمتلازمة_التنفسية_الحادة_الشديدة_النوع_2', 'bnwiki': 'গুরুতর_তীব্র_শ্বাসযন্ত্রীয়_রোগলক্ষণসমষ্টি_সৃষ্টিকারী_করোনাভাইরাস_২', 'hiwiki': '2019_नोवेल_कोरोनावायरस'}, 'label': 'SARS-CoV-2', 'description': 'strain of virus causing the ongoing pandemic of coronavirus disease 2019 (COVID-19)', 'text': 'SARS-CoV-2'}, {'qid': 'Q84263196', 'score': 0.8782671093940735, 'titles': {'enwiki': 'Coronavirus_disease_2019', 'dewiki': 'COVID-19', 'frwiki': 'Maladie_à_coronavirus_2019', 'eswiki': 'COVID-19', 'ruwiki': 'COVID-19', 'zhwiki': '2019冠状病毒病', 'ptwiki': 'COVID-19', 'arwiki': 'مرض_فيروس_كورونا_2019', 'bnwiki': 'করোনাভাইরাস_রোগ_২০১৯', 'hiwiki': 'कोरोनावायरस_रोग_2019'}, 'label': 'COVID-19', 'description': 'zoonotic respiratory syndrome and infectious disease in humans, caused by SARS coronavirus 2', 'text': 'COVID-19'}]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "dict_nn_score = defaultdict(list)\n",
    "for qid in list_qid_seed:\n",
    "    qid_nn = model.get_nearest_neighbors(qid,k=N_nn)\n",
    "    for score_tmp,qid_tmp in qid_nn:\n",
    "        dict_nn_score[qid_tmp] += [score_tmp] \n",
    "        \n",
    "list_nn_qid_tmp = []\n",
    "list_nn_score_tmp = []\n",
    "for qid_tmp,score_tmp in dict_nn_score.items():\n",
    "    list_nn_qid_tmp+=[qid_tmp]\n",
    "    list_nn_score_tmp += [max(score_tmp)]\n",
    "    \n",
    "indsort = np.argsort(list_nn_score_tmp)[::-1][:N_nn]\n",
    "list_nn_qid = [list_nn_qid_tmp[i] for i in indsort]\n",
    "list_nn_score = [list_nn_score_tmp[i] for i in indsort]\n",
    "list_items = [\n",
    "    {'qid': list_nn_qid[i],\n",
    "     'score': list_nn_score[i]\n",
    "    } \n",
    "    for i in range(N_nn) \n",
    "]\n",
    "list_items = items_add_titles(list_items,list_wikis = list_wikis)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter list: disambiguation, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_filter_notext(\n",
    "    list_items):\n",
    "    '''filter all items that have\n",
    "    - no English label\n",
    "    - no description\n",
    "    - no existing title in any of the selected wikis\n",
    "    '''\n",
    "    list_items_new = []\n",
    "    for item in list_items:\n",
    "        if item['text'] != '-':\n",
    "            list_items_new += [item]\n",
    "    return list_items_new\n",
    "\n",
    "def items_filter_disambiguation(\n",
    "    list_items,\n",
    "    list_keywords=['disambiguation']):\n",
    "    s = re.compile('|'.join(list_keywords))\n",
    "    list_items_new = []\n",
    "    for item in list_items:\n",
    "        label = item['label']+' '+item['description']\n",
    "        if s.search(label.lower()) is None:\n",
    "            list_items_new += [item]\n",
    "    return list_items_new\n",
    "\n",
    "def items_filter_keywords(\n",
    "    list_items,\n",
    "    list_keywords=[] ):\n",
    "    if len(list_keywords) == 0:\n",
    "        return list_items\n",
    "    else:\n",
    "        s = re.compile('|'.join(list_keywords))\n",
    "        list_items_new = []\n",
    "        for item in list_items:\n",
    "            label = item['label']\n",
    "            if s.search(label.lower()) is None:\n",
    "                list_items_new += [item]\n",
    "        return list_items_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q83554667', 'score': 0.8070012331008911, 'titles': {'enwiki': 'COVID-19_pandemic_lockdown_in_Hubei', 'dewiki': '-', 'frwiki': '-', 'eswiki': 'Encierro_de_Hubei_de_2020', 'ruwiki': '-', 'zhwiki': '2019冠狀病毒病中國大陸疫區封鎖措施', 'ptwiki': 'Lockdown_em_Hubei_em_2020', 'arwiki': 'عزل_ووهان_2020', 'bnwiki': '-', 'hiwiki': '-'}, 'label': '2020 Hubei lockdowns', 'description': 'Chinese quarantine effort in Hubei Province in response to the Wuhan coronavirus outbreak', 'text': '2020 Hubei lockdowns'}, {'qid': 'Q30314010', 'score': 0.7878186702728271, 'titles': {'enwiki': 'Social_distancing', 'dewiki': 'Räumliche_Distanzierung', 'frwiki': 'Distanciation_sociale', 'eswiki': 'Distanciamiento_físico', 'ruwiki': 'Социальное_дистанцирование', 'zhwiki': '保持社交距離', 'ptwiki': 'Distanciamento_social', 'arwiki': 'تباعد_اجتماعي', 'bnwiki': 'সামাজিক_দূরত্ব_স্থাপন', 'hiwiki': 'सामाजिक_दूरीकरण'}, 'label': 'social distancing', 'description': 'reduction of human social interaction in an effort to prevent the spread of infectious disease', 'text': 'social distancing'}]\n",
      "546\n"
     ]
    }
   ],
   "source": [
    "## filter if the label contains a keywords as substring\n",
    "list_items_filtered = list(list_items)\n",
    "list_items_filtered = items_filter_notext(list_items_filtered)\n",
    "list_items_filtered = items_filter_disambiguation(list_items_filtered)\n",
    "list_items_filtered = items_filter_keywords(list_items_filtered,list_keywords = list_keywords)\n",
    "list_items = list(list_items_filtered)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRedirects(page,project):\n",
    "    '''\n",
    "    get all redirects (upto 500)\n",
    "    for a given page \n",
    "    '''\n",
    "    base_url = 'https://%s.org/w/api.php?action=query&titles=%s&prop=redirects&rdlimit=500&format=json' % (project,page)\n",
    "    data = [p['title'] for p in list(requests.get(base_url).json()['query']['pages'].values())[0]['redirects']]\n",
    "    return data\n",
    "    \n",
    "\n",
    "def getViews(page,start,end,project):\n",
    "    \"\"\"\n",
    "    get pageviews using this API https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews \n",
    "    page: str (article name)\n",
    "    start: str start date YYYYMMDD (20200101)\n",
    "    end: str end date YYYYMMDD (20200103)\n",
    "    project: str, ex: en.wikipedia (project does not include .org)\n",
    "\n",
    "    \"\"\"\n",
    "    base_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/%s/all-access/all-agents/%s/daily/%s/%s\" % (project,page,start,end)\n",
    "    try:\n",
    "        data = requests.get(base_url).json()['items']\n",
    "        df = pd.DataFrame(data) [['views','timestamp']]\n",
    "        df.rename(columns={'views':page},inplace=True)\n",
    "    except KeyError:\n",
    "        ## no pageviews- we have to set 1 date with 0 counts\n",
    "        df = pd.DataFrame(columns=[page,'timestamp'],index=[0])\n",
    "        df.iloc[0,0] = 0\n",
    "        df.iloc[0,1] = start\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H')\n",
    "    df.set_index('timestamp',inplace=True)\n",
    "    return df\n",
    "\n",
    "def getViewsMultiples(pages,start,end,project):\n",
    "    \"\"\"\n",
    "    Get page views for a list of pages \n",
    "    pages: list (list of article's titles) ex: ['Chile','Brasil','Argentina']\n",
    "    start: str start date YYYYMMDD (20200101)\n",
    "    end: str end date YYYYMMDD (20200103)\n",
    "    project: str, ex: en.wikipedia (project does not include .org)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for page in pages:\n",
    "        try:\n",
    "            results.append(getViews(page,start,end,project))\n",
    "        except:\n",
    "            pass\n",
    "    return pd.concat(results,axis=1)\n",
    "\n",
    "def getViewsWithRedirects(page,start,end,project):\n",
    "    \"\"\"\n",
    "    Get all redirects going to 'page' and get pageviews for that article\n",
    "    page: str (article's title)\n",
    "    start: str start date YYYYMMDD (20200101)\n",
    "    end: str end date YYYYMMDD (20200103)\n",
    "    project: str, ex: en.wikipedia (project does not include .org)\n",
    "    \"\"\"\n",
    "    #start list of page\n",
    "    pages = [page]\n",
    "    try:\n",
    "        #get all redirects to page \n",
    "        redirects = getRedirects(page=page,project=project)     \n",
    "    except:\n",
    "        #if getRedirects gives an error, we assume that there no pages redirecting to page\n",
    "        redirects = []\n",
    "    pages.extend(redirects)\n",
    "    ## get pages views for all articles\n",
    "    results = getViewsMultiples(pages=pages,start=start,end=end,project=project)\n",
    "    #sum all pages views\n",
    "    results = pd.DataFrame(results.sum(axis=1))\n",
    "    results.rename(columns={0:page},inplace=True)\n",
    "    return results      \n",
    "\n",
    "def items_add_pageviews(list_items,start,end,N_max = 10):\n",
    "    '''\n",
    "    for a list of records [{'titles':{'enwiki':'Germany','dewiki':'Deutschland'}}, {}]\n",
    "    add pageviews\n",
    "    '''\n",
    "    \n",
    "    for i_item,item in enumerate(list_items[:N_max]):\n",
    "        dict_pageviews = {}\n",
    "        for wiki in list_wikis:\n",
    "            project = '%s.wikipedia' % wiki.replace('wiki','')\n",
    "            page = item['titles'][wiki]\n",
    "            if page == '-':\n",
    "                views = np.nan\n",
    "            else:\n",
    "                ## get pageviews per day\n",
    "                views_df = getViewsWithRedirects(page,start,end,project)\n",
    "                views = int(views_df.sum())\n",
    "            ## sum pageviews\n",
    "            dict_pageviews[wiki] = views\n",
    "        list_items[i_item]['pageviews'] = dict_pageviews\n",
    "    return list_items[:N_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q83554667', 'score': 0.8070012331008911, 'titles': {'enwiki': 'COVID-19_pandemic_lockdown_in_Hubei', 'dewiki': '-', 'frwiki': '-', 'eswiki': 'Encierro_de_Hubei_de_2020', 'ruwiki': '-', 'zhwiki': '2019冠狀病毒病中國大陸疫區封鎖措施', 'ptwiki': 'Lockdown_em_Hubei_em_2020', 'arwiki': 'عزل_ووهان_2020', 'bnwiki': '-', 'hiwiki': '-'}, 'label': '2020 Hubei lockdowns', 'description': 'Chinese quarantine effort in Hubei Province in response to the Wuhan coronavirus outbreak', 'text': '2020 Hubei lockdowns', 'pageviews': {'enwiki': 29236, 'dewiki': nan, 'frwiki': nan, 'eswiki': 1135, 'ruwiki': nan, 'zhwiki': 3538, 'ptwiki': 400, 'arwiki': 133, 'bnwiki': nan, 'hiwiki': nan}}, {'qid': 'Q30314010', 'score': 0.7878186702728271, 'titles': {'enwiki': 'Social_distancing', 'dewiki': 'Räumliche_Distanzierung', 'frwiki': 'Distanciation_sociale', 'eswiki': 'Distanciamiento_físico', 'ruwiki': 'Социальное_дистанцирование', 'zhwiki': '保持社交距離', 'ptwiki': 'Distanciamento_social', 'arwiki': 'تباعد_اجتماعي', 'bnwiki': 'সামাজিক_দূরত্ব_স্থাপন', 'hiwiki': 'सामाजिक_दूरीकरण'}, 'label': 'social distancing', 'description': 'reduction of human social interaction in an effort to prevent the spread of infectious disease', 'text': 'social distancing', 'pageviews': {'enwiki': 50533, 'dewiki': 4459, 'frwiki': 13688, 'eswiki': 3123, 'ruwiki': 1841, 'zhwiki': 1437, 'ptwiki': 871, 'arwiki': 629, 'bnwiki': 602, 'hiwiki': 393}}]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "start = str(day_start).replace('-','')\n",
    "end = str(day_start+datetime.timedelta(days=n_days)).replace('-','')\n",
    "list_items = items_add_pageviews(list_items,start,end,N_max=N_max)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(qid,p_min=0.1):\n",
    "    api_url_base = 'https://tools.wmflabs.org/wiki-topic/api/v1/wikidata/topic'\n",
    "    params = {\n",
    "        'qid':qid,\n",
    "         'threshold':0.1\n",
    "    }\n",
    "    ## query api: https://tools.wmflabs.org/wiki-topic/\n",
    "    response = requests.get( api_url_base,params=params)\n",
    "    result=json.loads(response.text)   \n",
    "    ## get scores (topic-probabilities) and topics\n",
    "    scores = [h['score'] for h in result['results']]\n",
    "    topics = [h['topic'] for h in result['results']]\n",
    "    ## get topic w maximum probability (if p exceeds p_min)\n",
    "    ind_max = np.argmax(scores)\n",
    "    score_max = scores[ind_max]\n",
    "    if score_max >= p_min:\n",
    "        topic = topics[ind_max]\n",
    "    else:\n",
    "        topic = '-'\n",
    "    return topic\n",
    "    \n",
    "def items_addTopics(list_items):\n",
    "    for i_item,item in enumerate(list_items):\n",
    "        qid = item['qid']\n",
    "        topic = getTopic(qid)\n",
    "        list_items[i_item]['topic'] = topic\n",
    "    return list_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q83554667', 'score': 0.8070012331008911, 'titles': {'enwiki': 'COVID-19_pandemic_lockdown_in_Hubei', 'dewiki': '-', 'frwiki': '-', 'eswiki': 'Encierro_de_Hubei_de_2020', 'ruwiki': '-', 'zhwiki': '2019冠狀病毒病中國大陸疫區封鎖措施', 'ptwiki': 'Lockdown_em_Hubei_em_2020', 'arwiki': 'عزل_ووهان_2020', 'bnwiki': '-', 'hiwiki': '-'}, 'label': '2020 Hubei lockdowns', 'description': 'Chinese quarantine effort in Hubei Province in response to the Wuhan coronavirus outbreak', 'text': '2020 Hubei lockdowns', 'pageviews': {'enwiki': 29236, 'dewiki': nan, 'frwiki': nan, 'eswiki': 1135, 'ruwiki': nan, 'zhwiki': 3538, 'ptwiki': 400, 'arwiki': 133, 'bnwiki': nan, 'hiwiki': nan}, 'topic': 'Geography.Regions.Asia.Asia*'}, {'qid': 'Q30314010', 'score': 0.7878186702728271, 'titles': {'enwiki': 'Social_distancing', 'dewiki': 'Räumliche_Distanzierung', 'frwiki': 'Distanciation_sociale', 'eswiki': 'Distanciamiento_físico', 'ruwiki': 'Социальное_дистанцирование', 'zhwiki': '保持社交距離', 'ptwiki': 'Distanciamento_social', 'arwiki': 'تباعد_اجتماعي', 'bnwiki': 'সামাজিক_দূরত্ব_স্থাপন', 'hiwiki': 'सामाजिक_दूरीकरण'}, 'label': 'social distancing', 'description': 'reduction of human social interaction in an effort to prevent the spread of infectious disease', 'text': 'social distancing', 'pageviews': {'enwiki': 50533, 'dewiki': 4459, 'frwiki': 13688, 'eswiki': 3123, 'ruwiki': 1841, 'zhwiki': 1437, 'ptwiki': 871, 'arwiki': 629, 'bnwiki': 602, 'hiwiki': 393}, 'topic': 'STEM.STEM*'}]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "list_items = items_addTopics(list_items)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikidata-item</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>topic</th>\n",
       "      <th>enwiki:title</th>\n",
       "      <th>enwiki:views</th>\n",
       "      <th>dewiki:title</th>\n",
       "      <th>dewiki:views</th>\n",
       "      <th>frwiki:title</th>\n",
       "      <th>frwiki:views</th>\n",
       "      <th>...</th>\n",
       "      <th>zhwiki:title</th>\n",
       "      <th>zhwiki:views</th>\n",
       "      <th>ptwiki:title</th>\n",
       "      <th>ptwiki:views</th>\n",
       "      <th>arwiki:title</th>\n",
       "      <th>arwiki:views</th>\n",
       "      <th>bnwiki:title</th>\n",
       "      <th>bnwiki:views</th>\n",
       "      <th>hiwiki:title</th>\n",
       "      <th>hiwiki:views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[wikidata:Q83554667|Q83554667]]</td>\n",
       "      <td>2020 Hubei lockdowns</td>\n",
       "      <td>0.807</td>\n",
       "      <td>Geography.Regions.Asia.Asia*</td>\n",
       "      <td>[[en:COVID-19_pandemic_lockdown_in_Hubei|COVID...</td>\n",
       "      <td>29236</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>[[zh:2019冠狀病毒病中國大陸疫區封鎖措施|2019冠狀病毒病中國大陸疫區封鎖措施]]</td>\n",
       "      <td>3538</td>\n",
       "      <td>[[pt:Lockdown_em_Hubei_em_2020|Lockdown em Hub...</td>\n",
       "      <td>&lt;500</td>\n",
       "      <td>[[ar:عزل_ووهان_2020|عزل ووهان 2020]]</td>\n",
       "      <td>&lt;500</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[wikidata:Q30314010|Q30314010]]</td>\n",
       "      <td>social distancing</td>\n",
       "      <td>0.788</td>\n",
       "      <td>STEM.STEM*</td>\n",
       "      <td>[[en:Social_distancing|Social distancing]]</td>\n",
       "      <td>50533</td>\n",
       "      <td>[[de:Räumliche_Distanzierung|Räumliche Distanz...</td>\n",
       "      <td>4459</td>\n",
       "      <td>[[fr:Distanciation_sociale|Distanciation socia...</td>\n",
       "      <td>13688</td>\n",
       "      <td>...</td>\n",
       "      <td>[[zh:保持社交距離|保持社交距離]]</td>\n",
       "      <td>1437</td>\n",
       "      <td>[[pt:Distanciamento_social|Distanciamento soci...</td>\n",
       "      <td>871</td>\n",
       "      <td>[[ar:تباعد_اجتماعي|تباعد اجتماعي]]</td>\n",
       "      <td>629</td>\n",
       "      <td>[[bn:সামাজিক_দূরত্ব_স্থাপন|সামাজিক দূরত্ব স্থা...</td>\n",
       "      <td>602</td>\n",
       "      <td>[[hi:सामाजिक_दूरीकरण|सामाजिक दूरीकरण]]</td>\n",
       "      <td>&lt;500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[wikidata:Q103177|Q103177]]</td>\n",
       "      <td>severe acute respiratory syndrome</td>\n",
       "      <td>0.785</td>\n",
       "      <td>Compilation.List_Disambig</td>\n",
       "      <td>[[en:Severe_acute_respiratory_syndrome|Severe ...</td>\n",
       "      <td>96443</td>\n",
       "      <td>[[de:Schweres_akutes_Atemwegssyndrom|Schweres ...</td>\n",
       "      <td>11759</td>\n",
       "      <td>[[fr:Syndrome_respiratoire_aigu_sévère|Syndrom...</td>\n",
       "      <td>9816</td>\n",
       "      <td>...</td>\n",
       "      <td>[[zh:嚴重急性呼吸系統綜合症|嚴重急性呼吸系統綜合症]]</td>\n",
       "      <td>7870</td>\n",
       "      <td>[[pt:Síndrome_respiratória_aguda_grave|Síndrom...</td>\n",
       "      <td>9713</td>\n",
       "      <td>[[ar:متلازمة_تنفسية_حادة_وخيمة|متلازمة تنفسية ...</td>\n",
       "      <td>4472</td>\n",
       "      <td>[[bn:গুরুতর_তীব্র_শ্বাসযন্ত্রীয়_রোগলক্ষণসমষ্ট...</td>\n",
       "      <td>&lt;500</td>\n",
       "      <td>[[hi:सिवियर_एक्यूट_रेस्पिरेटरी_सिंड्रोम|सिवियर...</td>\n",
       "      <td>983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[wikidata:Q12184|Q12184]]</td>\n",
       "      <td>pandemic</td>\n",
       "      <td>0.770</td>\n",
       "      <td>STEM.STEM*</td>\n",
       "      <td>[[en:Pandemic|Pandemic]]</td>\n",
       "      <td>139758</td>\n",
       "      <td>[[de:Pandemie|Pandemie]]</td>\n",
       "      <td>59428</td>\n",
       "      <td>[[fr:Pandémie|Pandémie]]</td>\n",
       "      <td>22840</td>\n",
       "      <td>...</td>\n",
       "      <td>[[zh:瘟疫|瘟疫]]</td>\n",
       "      <td>7885</td>\n",
       "      <td>[[pt:Pandemia|Pandemia]]</td>\n",
       "      <td>27378</td>\n",
       "      <td>[[ar:جائحة|جائحة]]</td>\n",
       "      <td>6682</td>\n",
       "      <td>[[bn:বৈশ্বিক_মহামারী|বৈশ্বিক মহামারী]]</td>\n",
       "      <td>791</td>\n",
       "      <td>[[hi:विश्वमारी|विश्वमारी]]</td>\n",
       "      <td>4254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[wikidata:Q84497471|Q84497471]]</td>\n",
       "      <td>Li Wenliang</td>\n",
       "      <td>0.756</td>\n",
       "      <td>Culture.Biography.Biography*</td>\n",
       "      <td>[[en:Li_Wenliang|Li Wenliang]]</td>\n",
       "      <td>33321</td>\n",
       "      <td>[[de:Li_Wenliang|Li Wenliang]]</td>\n",
       "      <td>4772</td>\n",
       "      <td>[[fr:Li_Wenliang|Li Wenliang]]</td>\n",
       "      <td>1909</td>\n",
       "      <td>...</td>\n",
       "      <td>[[zh:李文亮|李文亮]]</td>\n",
       "      <td>7760</td>\n",
       "      <td>[[pt:Li_Wenliang_(médico)|Li Wenliang (médico)]]</td>\n",
       "      <td>983</td>\n",
       "      <td>[[ar:لي_وين_ليانغ|لي وين ليانغ]]</td>\n",
       "      <td>526</td>\n",
       "      <td>[[bn:লি_ওয়েনলিয়াং|লি ওয়েনলিয়াং]]</td>\n",
       "      <td>&lt;500</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Wikidata-item                              label  score  \\\n",
       "1  [[wikidata:Q83554667|Q83554667]]               2020 Hubei lockdowns  0.807   \n",
       "2  [[wikidata:Q30314010|Q30314010]]                  social distancing  0.788   \n",
       "3      [[wikidata:Q103177|Q103177]]  severe acute respiratory syndrome  0.785   \n",
       "4        [[wikidata:Q12184|Q12184]]                           pandemic  0.770   \n",
       "5  [[wikidata:Q84497471|Q84497471]]                        Li Wenliang  0.756   \n",
       "\n",
       "                          topic  \\\n",
       "1  Geography.Regions.Asia.Asia*   \n",
       "2                    STEM.STEM*   \n",
       "3     Compilation.List_Disambig   \n",
       "4                    STEM.STEM*   \n",
       "5  Culture.Biography.Biography*   \n",
       "\n",
       "                                        enwiki:title enwiki:views  \\\n",
       "1  [[en:COVID-19_pandemic_lockdown_in_Hubei|COVID...        29236   \n",
       "2         [[en:Social_distancing|Social distancing]]        50533   \n",
       "3  [[en:Severe_acute_respiratory_syndrome|Severe ...        96443   \n",
       "4                           [[en:Pandemic|Pandemic]]       139758   \n",
       "5                     [[en:Li_Wenliang|Li Wenliang]]        33321   \n",
       "\n",
       "                                        dewiki:title dewiki:views  \\\n",
       "1                                                  -            -   \n",
       "2  [[de:Räumliche_Distanzierung|Räumliche Distanz...         4459   \n",
       "3  [[de:Schweres_akutes_Atemwegssyndrom|Schweres ...        11759   \n",
       "4                           [[de:Pandemie|Pandemie]]        59428   \n",
       "5                     [[de:Li_Wenliang|Li Wenliang]]         4772   \n",
       "\n",
       "                                        frwiki:title frwiki:views  ...  \\\n",
       "1                                                  -            -  ...   \n",
       "2  [[fr:Distanciation_sociale|Distanciation socia...        13688  ...   \n",
       "3  [[fr:Syndrome_respiratoire_aigu_sévère|Syndrom...         9816  ...   \n",
       "4                           [[fr:Pandémie|Pandémie]]        22840  ...   \n",
       "5                     [[fr:Li_Wenliang|Li Wenliang]]         1909  ...   \n",
       "\n",
       "                                     zhwiki:title zhwiki:views  \\\n",
       "1  [[zh:2019冠狀病毒病中國大陸疫區封鎖措施|2019冠狀病毒病中國大陸疫區封鎖措施]]         3538   \n",
       "2                            [[zh:保持社交距離|保持社交距離]]         1437   \n",
       "3                  [[zh:嚴重急性呼吸系統綜合症|嚴重急性呼吸系統綜合症]]         7870   \n",
       "4                                    [[zh:瘟疫|瘟疫]]         7885   \n",
       "5                                  [[zh:李文亮|李文亮]]         7760   \n",
       "\n",
       "                                        ptwiki:title ptwiki:views  \\\n",
       "1  [[pt:Lockdown_em_Hubei_em_2020|Lockdown em Hub...         <500   \n",
       "2  [[pt:Distanciamento_social|Distanciamento soci...          871   \n",
       "3  [[pt:Síndrome_respiratória_aguda_grave|Síndrom...         9713   \n",
       "4                           [[pt:Pandemia|Pandemia]]        27378   \n",
       "5   [[pt:Li_Wenliang_(médico)|Li Wenliang (médico)]]          983   \n",
       "\n",
       "                                        arwiki:title arwiki:views  \\\n",
       "1               [[ar:عزل_ووهان_2020|عزل ووهان 2020]]         <500   \n",
       "2                 [[ar:تباعد_اجتماعي|تباعد اجتماعي]]          629   \n",
       "3  [[ar:متلازمة_تنفسية_حادة_وخيمة|متلازمة تنفسية ...         4472   \n",
       "4                                 [[ar:جائحة|جائحة]]         6682   \n",
       "5                   [[ar:لي_وين_ليانغ|لي وين ليانغ]]          526   \n",
       "\n",
       "                                        bnwiki:title bnwiki:views  \\\n",
       "1                                                  -            -   \n",
       "2  [[bn:সামাজিক_দূরত্ব_স্থাপন|সামাজিক দূরত্ব স্থা...          602   \n",
       "3  [[bn:গুরুতর_তীব্র_শ্বাসযন্ত্রীয়_রোগলক্ষণসমষ্ট...         <500   \n",
       "4             [[bn:বৈশ্বিক_মহামারী|বৈশ্বিক মহামারী]]          791   \n",
       "5               [[bn:লি_ওয়েনলিয়াং|লি ওয়েনলিয়াং]]         <500   \n",
       "\n",
       "                                        hiwiki:title hiwiki:views  \n",
       "1                                                  -            -  \n",
       "2             [[hi:सामाजिक_दूरीकरण|सामाजिक दूरीकरण]]         <500  \n",
       "3  [[hi:सिवियर_एक्यूट_रेस्पिरेटरी_सिंड्रोम|सिवियर...          983  \n",
       "4                         [[hi:विश्वमारी|विश्वमारी]]         4254  \n",
       "5                                                  -            -  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "if formatted == True:\n",
    "    df['Wikidata-item'] = [ '[[wikidata:{0}|{0}]]'.format(h['qid']) for h in list_items ]\n",
    "else:\n",
    "    df['Wikidata-item'] = [ '{0}'.format(h['qid']) for h in list_items ]\n",
    "df['label'] = [ h['text'] for h in list_items ]\n",
    "df['score'] = [ '%.3f'%(h['score']) for h in list_items ]\n",
    "df['topic'] = [ h['topic'] for h in list_items ]\n",
    "for wiki in list_wikis:\n",
    "    if formatted == True:\n",
    "        df['%s:title'%(wiki)] = [  '[[%s:%s|%s]]'%(wiki[:-4],h['titles'][wiki],h['titles'][wiki].replace('_',' ')) if h['titles'][wiki]!='-' else '-'  for h in list_items ]\n",
    "    else:\n",
    "        df['%s:title'%(wiki)] = [  '%s'%(h['titles'][wiki].replace('_',' ')) if h['titles'][wiki]!='-' else '-'  for h in list_items ]\n",
    "\n",
    "    df['%s:views'%wiki] = [ '%s'%(h['pageviews'][wiki]) if (h['pageviews'][wiki])>=500 else '<500' if np.isnan((h['pageviews'][wiki]))==False else '-'   for h in list_items ]\n",
    "\n",
    "df.index = np.arange(1, len(df) + 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_save = os.path.join(\n",
    "    os.pardir,\n",
    "    'output',\n",
    "    'lists'\n",
    "    'reading-sessions-covid_list-%s_N%s_%s_%s.csv'%(\n",
    "        '-'.join(list_qid_seed),\n",
    "        N_max,\n",
    "        str(day_start),\n",
    "        str(day_start+datetime.timedelta(days=n_days-1))\n",
    "    )\n",
    ")\n",
    "df.to_csv(filename_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gensim",
   "language": "python",
   "name": "venv_gensim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
